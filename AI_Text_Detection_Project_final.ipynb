{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74a582a0-b2ea-496d-b321-2161307f2276",
   "metadata": {},
   "source": [
    "#  AI Text Detection Project\n",
    "This notebook implements the detection of AI-generated texts using embeddings and machine learning techniques. The steps include:\n",
    "\n",
    "1. Loading the dataset.\n",
    "2. Preprocessing the text data.\n",
    "3. Using pre-computed embeddings to save time.\n",
    "4. Training and evaluating classification models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "536c3e4d-ae39-4cf4-8969-ceefe44e6dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (29145, 2)\n",
      "                                                text  generated\n",
      "0  Car-free cities have become a subject of incre...          1\n",
      "1  Car Free Cities  Car-free cities, a concept ga...          1\n",
      "2    A Sustainable Urban Future  Car-free cities ...          1\n",
      "3    Pioneering Sustainable Urban Living  In an e...          1\n",
      "4    The Path to Sustainable Urban Living  In an ...          1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"R:/text_detection_project/datasets/Training_Essay_Data.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Inspect the dataset\n",
    "print(f\"Dataset Shape: {data.shape}\")\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7a0de7c-1608-44a7-944a-415e07e3d610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  Car-free cities have become a subject of incre...   \n",
      "1  Car Free Cities  Car-free cities, a concept ga...   \n",
      "2    A Sustainable Urban Future  Car-free cities ...   \n",
      "3    Pioneering Sustainable Urban Living  In an e...   \n",
      "4    The Path to Sustainable Urban Living  In an ...   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  carfree cities have become a subject of increa...  \n",
      "1  car free cities  carfree cities a concept gain...  \n",
      "2  a sustainable urban future  carfree cities are...  \n",
      "3  pioneering sustainable urban living  in an era...  \n",
      "4  the path to sustainable urban living  in an ag...  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Text Preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Remove special characters, numbers, and punctuations\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower().strip()\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to the text column\n",
    "data['cleaned_text'] = data['text'].apply(preprocess_text)\n",
    "\n",
    "# Display a few samples of the cleaned text\n",
    "print(data[['text', 'cleaned_text']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3525a80f-817c-4a92-b408-63e744ef0179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Embeddings: (32256, 768)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Path to the saved embeddings\n",
    "embeddings_path = \"R:/text_detection_project/datasets/bert_embeddings_combined.npy\"\n",
    "\n",
    "# Load the combined embeddings\n",
    "embeddings = np.load(embeddings_path, allow_pickle=True)\n",
    "\n",
    "# Check the shape of the embeddings\n",
    "print(f\"Shape of Embeddings: {embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e519ee96-7959-44b1-8781-e5f9050e1331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch detected: Dataset rows = 29145, Embedding rows = 32256\n",
      "Dataset trimmed to match embeddings: 29145 rows.\n",
      "Final Dataset Shape: (29145, 3)\n"
     ]
    }
   ],
   "source": [
    "# Ensure the dataset and embeddings have the same number of rows\n",
    "if len(data) != len(embeddings):\n",
    "    print(f\"Mismatch detected: Dataset rows = {len(data)}, Embedding rows = {len(embeddings)}\")\n",
    "    data = data.iloc[:len(embeddings)]\n",
    "    print(f\"Dataset trimmed to match embeddings: {len(data)} rows.\")\n",
    "\n",
    "# Display the final dataset shape\n",
    "print(f\"Final Dataset Shape: {data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38d56201-e07a-4b8f-a21e-db187a15fc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (23316, 768), (23316,)\n",
      "Testing set: (5829, 768), (5829,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Features (embeddings) and labels\n",
    "X = embeddings[:len(data)]  # Ensure embeddings match the trimmed dataset\n",
    "y = data['generated'].values  # Assuming 'generated' column contains labels\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shape of the splits\n",
    "print(f\"Training set: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Testing set: {X_test.shape}, {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7186c9f1-09da-4d54-a633-d6e8683b0a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.79      0.79      3539\n",
      "           1       0.67      0.65      0.66      2290\n",
      "\n",
      "    accuracy                           0.74      5829\n",
      "   macro avg       0.72      0.72      0.72      5829\n",
      "weighted avg       0.74      0.74      0.74      5829\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2807  732]\n",
      " [ 802 1488]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Initialize and train the Logistic Regression model\n",
    "clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa8b364-5d94-45b6-a6fd-e1e7585af2b0",
   "metadata": {},
   "source": [
    "# Neural Network Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c48f146b-613e-4b2c-8a40-daaefede0aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit-Learn version: 1.6.0\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(\"Scikit-Learn version:\", sklearn.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4efa89d4-9ae6-4fde-ac2b-8f9aa1ea400e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "583/583 [==============================] - 6s 9ms/step - loss: 0.5146 - accuracy: 0.7175 - val_loss: 0.4673 - val_accuracy: 0.7481\n",
      "Epoch 2/10\n",
      "583/583 [==============================] - 5s 8ms/step - loss: 0.4690 - accuracy: 0.7413 - val_loss: 0.4591 - val_accuracy: 0.7556\n",
      "Epoch 3/10\n",
      "583/583 [==============================] - 5s 9ms/step - loss: 0.4564 - accuracy: 0.7524 - val_loss: 0.4613 - val_accuracy: 0.7624\n",
      "Epoch 4/10\n",
      "583/583 [==============================] - 5s 8ms/step - loss: 0.4511 - accuracy: 0.7555 - val_loss: 0.4468 - val_accuracy: 0.7594\n",
      "Epoch 5/10\n",
      "583/583 [==============================] - 5s 9ms/step - loss: 0.4452 - accuracy: 0.7595 - val_loss: 0.4433 - val_accuracy: 0.7676\n",
      "Epoch 6/10\n",
      "583/583 [==============================] - 5s 9ms/step - loss: 0.4389 - accuracy: 0.7640 - val_loss: 0.4459 - val_accuracy: 0.7564\n",
      "Epoch 7/10\n",
      "583/583 [==============================] - 5s 9ms/step - loss: 0.4350 - accuracy: 0.7660 - val_loss: 0.4450 - val_accuracy: 0.7556\n",
      "Epoch 8/10\n",
      "583/583 [==============================] - 5s 8ms/step - loss: 0.4355 - accuracy: 0.7622 - val_loss: 0.4421 - val_accuracy: 0.7633\n",
      "Epoch 9/10\n",
      "583/583 [==============================] - 5s 9ms/step - loss: 0.4321 - accuracy: 0.7658 - val_loss: 0.4385 - val_accuracy: 0.7672\n",
      "Epoch 10/10\n",
      "583/583 [==============================] - 5s 9ms/step - loss: 0.4290 - accuracy: 0.7659 - val_loss: 0.4400 - val_accuracy: 0.7633\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4345 - accuracy: 0.7694\n",
      "Test Accuracy: 0.77\n",
      "183/183 [==============================] - 0s 2ms/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.90      0.83      3539\n",
      "           1       0.78      0.57      0.66      2290\n",
      "\n",
      "    accuracy                           0.77      5829\n",
      "   macro avg       0.77      0.73      0.74      5829\n",
      "weighted avg       0.77      0.77      0.76      5829\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3179  360]\n",
      " [ 984 1306]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Define the Neural Network architecture\n",
    "model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(768,)),  # First hidden layer\n",
    "    Dropout(0.3),                                       # Dropout for regularization\n",
    "    Dense(256, activation='relu'),                      # Second hidden layer\n",
    "    Dropout(0.3),                                       # Dropout for regularization\n",
    "    Dense(1, activation='sigmoid')                      # Output layer (binary classification)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_nn = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Classification report and confusion matrix\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_nn))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_nn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3276c869-cb2b-45ff-a94a-feb052ea2301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "583/583 [==============================] - 5s 9ms/step - loss: 0.4276 - accuracy: 0.7662 - val_loss: 0.4386 - val_accuracy: 0.7633\n",
      "Epoch 2/20\n",
      "583/583 [==============================] - 5s 9ms/step - loss: 0.4277 - accuracy: 0.7679 - val_loss: 0.4389 - val_accuracy: 0.7599\n",
      "Epoch 3/20\n",
      "583/583 [==============================] - 5s 9ms/step - loss: 0.4243 - accuracy: 0.7724 - val_loss: 0.4424 - val_accuracy: 0.7669\n",
      "Epoch 4/20\n",
      "583/583 [==============================] - 5s 9ms/step - loss: 0.4205 - accuracy: 0.7733 - val_loss: 0.4428 - val_accuracy: 0.7590\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Model checkpoint to save the best model\n",
    "model_checkpoint = ModelCheckpoint('best_nn_model.h5', save_best_only=True, monitor='val_loss')\n",
    "\n",
    "# Retrain the model with callbacks\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=20,  # You can experiment with more epochs\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10c99a98-b04e-43df-8314-8c5751012f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4342 - accuracy: 0.7641\n",
      "Test Accuracy: 0.76\n",
      "183/183 [==============================] - 1s 3ms/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.89      0.82      3539\n",
      "           1       0.77      0.57      0.66      2290\n",
      "\n",
      "    accuracy                           0.76      5829\n",
      "   macro avg       0.77      0.73      0.74      5829\n",
      "weighted avg       0.76      0.76      0.76      5829\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3141  398]\n",
      " [ 977 1313]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the best saved model\n",
    "best_model = load_model('best_nn_model.h5')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = best_model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_nn_refined = (best_model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Classification report and confusion matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_nn_refined))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_nn_refined))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fd96e60-3181-44cb-9efb-c595de322357",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [32256, 29145]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m y \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues  \u001b[38;5;66;03m# Ensure this matches the dataset structure\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Split into training and testing sets\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbert_embeddings_combined\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining data shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting data shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_test\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mR:\\text_detection_project\\text_detection_project_introtoAI\\tensorflow_env\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32mR:\\text_detection_project\\text_detection_project_introtoAI\\tensorflow_env\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2848\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2845\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_arrays \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2846\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one array required as input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2848\u001b[0m arrays \u001b[38;5;241m=\u001b[39m \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2850\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   2851\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2852\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[0;32m   2853\u001b[0m )\n",
      "File \u001b[1;32mR:\\text_detection_project\\text_detection_project_introtoAI\\tensorflow_env\\lib\\site-packages\\sklearn\\utils\\validation.py:532\u001b[0m, in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[0;32m    503\u001b[0m \n\u001b[0;32m    504\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;124;03m[[1, 2, 3], array([2, 3, 4]), None, <...Sparse...dtype 'int64'...shape (3, 1)>]\u001b[39;00m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    531\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[1;32m--> 532\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mR:\\text_detection_project\\text_detection_project_introtoAI\\tensorflow_env\\lib\\site-packages\\sklearn\\utils\\validation.py:475\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    473\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 475\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    476\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    477\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    478\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [32256, 29145]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load combined BERT embeddings\n",
    "embedding_path = \"R:/text_detection_project/datasets/bert_embeddings_combined.npy\"\n",
    "bert_embeddings_combined = np.load(embedding_path)\n",
    "\n",
    "# Load the target labels\n",
    "y = data['generated'].values  # Ensure this matches the dataset structure\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(bert_embeddings_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34402218-40ea-4ed4-a9ef-f6305ceec614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (23316, 768)\n",
      "Testing data shape: (5829, 768)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(bert_embeddings_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec8f9b3b-e42f-4918-a5be-7c6414996f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "583/583 [==============================] - 6s 9ms/step - loss: 0.5116 - accuracy: 0.7197 - val_loss: 0.4620 - val_accuracy: 0.7569\n",
      "Epoch 2/20\n",
      "583/583 [==============================] - 5s 8ms/step - loss: 0.4700 - accuracy: 0.7432 - val_loss: 0.4612 - val_accuracy: 0.7489\n",
      "Epoch 3/20\n",
      "583/583 [==============================] - 5s 9ms/step - loss: 0.4567 - accuracy: 0.7550 - val_loss: 0.4556 - val_accuracy: 0.7665\n",
      "Epoch 4/20\n",
      "583/583 [==============================] - 5s 9ms/step - loss: 0.4521 - accuracy: 0.7566 - val_loss: 0.4487 - val_accuracy: 0.7596\n",
      "Epoch 5/20\n",
      "583/583 [==============================] - 5s 9ms/step - loss: 0.4454 - accuracy: 0.7602 - val_loss: 0.4505 - val_accuracy: 0.7543\n",
      "Epoch 6/20\n",
      "583/583 [==============================] - 5s 9ms/step - loss: 0.4396 - accuracy: 0.7629 - val_loss: 0.4456 - val_accuracy: 0.7616\n",
      "Epoch 7/20\n",
      "583/583 [==============================] - 5s 9ms/step - loss: 0.4362 - accuracy: 0.7643 - val_loss: 0.4476 - val_accuracy: 0.7605\n",
      "Epoch 8/20\n",
      "583/583 [==============================] - 5s 9ms/step - loss: 0.4331 - accuracy: 0.7654 - val_loss: 0.4433 - val_accuracy: 0.7592\n",
      "Epoch 9/20\n",
      "583/583 [==============================] - 5s 9ms/step - loss: 0.4332 - accuracy: 0.7673 - val_loss: 0.4429 - val_accuracy: 0.7556\n",
      "Epoch 10/20\n",
      "583/583 [==============================] - 5s 9ms/step - loss: 0.4284 - accuracy: 0.7700 - val_loss: 0.4465 - val_accuracy: 0.7547\n",
      "Epoch 11/20\n",
      "583/583 [==============================] - 5s 9ms/step - loss: 0.4277 - accuracy: 0.7658 - val_loss: 0.4485 - val_accuracy: 0.7607\n",
      "Epoch 12/20\n",
      "583/583 [==============================] - 5s 9ms/step - loss: 0.4258 - accuracy: 0.7690 - val_loss: 0.4491 - val_accuracy: 0.7532\n",
      "Epoch 13/20\n",
      "583/583 [==============================] - 5s 9ms/step - loss: 0.4232 - accuracy: 0.7703 - val_loss: 0.4393 - val_accuracy: 0.7665\n",
      "Epoch 14/20\n",
      "583/583 [==============================] - 5s 9ms/step - loss: 0.4221 - accuracy: 0.7703 - val_loss: 0.4397 - val_accuracy: 0.7614\n",
      "Epoch 15/20\n",
      "583/583 [==============================] - 5s 9ms/step - loss: 0.4212 - accuracy: 0.7710 - val_loss: 0.4516 - val_accuracy: 0.7558\n",
      "Epoch 16/20\n",
      "583/583 [==============================] - 5s 9ms/step - loss: 0.4198 - accuracy: 0.7719 - val_loss: 0.4469 - val_accuracy: 0.7498\n",
      "Epoch 17/20\n",
      "583/583 [==============================] - 5s 9ms/step - loss: 0.4163 - accuracy: 0.7740 - val_loss: 0.4393 - val_accuracy: 0.7663\n",
      "Epoch 18/20\n",
      "583/583 [==============================] - 5s 9ms/step - loss: 0.4159 - accuracy: 0.7726 - val_loss: 0.4463 - val_accuracy: 0.7596\n",
      "Epoch 19/20\n",
      "583/583 [==============================] - 6s 10ms/step - loss: 0.4116 - accuracy: 0.7728 - val_loss: 0.4365 - val_accuracy: 0.7650\n",
      "Epoch 20/20\n",
      "583/583 [==============================] - 6s 10ms/step - loss: 0.4112 - accuracy: 0.7751 - val_loss: 0.4445 - val_accuracy: 0.7631\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4372 - accuracy: 0.7610\n",
      "Test Accuracy: 0.76\n",
      "183/183 [==============================] - 0s 2ms/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.89      0.82      3539\n",
      "           1       0.77      0.55      0.65      2290\n",
      "\n",
      "    accuracy                           0.76      5829\n",
      "   macro avg       0.76      0.72      0.73      5829\n",
      "weighted avg       0.76      0.76      0.75      5829\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3166  373]\n",
      " [1020 1270]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Define the Neural Network architecture\n",
    "model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(768,)),  # First hidden layer\n",
    "    Dropout(0.3),                                       # Dropout for regularization\n",
    "    Dense(256, activation='relu'),                      # Second hidden layer\n",
    "    Dropout(0.3),                                       # Dropout for regularization\n",
    "    Dense(1, activation='sigmoid')                      # Output layer (binary classification)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_nn = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Classification report and confusion matrix\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_nn))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_nn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c18b9bff-0abb-4af3-aae5-b27df53bea31",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Test each sample text\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m sample_texts:\n\u001b[1;32m---> 31\u001b[0m     predicted_class, confidence \u001b[38;5;241m=\u001b[39m predict_text(text, model, \u001b[43mtokenizer\u001b[49m, model)\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted Class: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (Confidence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfidence\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to preprocess and generate BERT embeddings for a single input\n",
    "def predict_text(text, model, tokenizer, bert_model):\n",
    "    # Preprocess text\n",
    "    cleaned_text = clean_text(text)  # Reuse the clean_text function\n",
    "    lemmatized_text = ' '.join(lemmatize_tokens(word_tokenize(cleaned_text)))\n",
    "\n",
    "    # Generate BERT embeddings\n",
    "    inputs = tokenizer(lemmatized_text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "    embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "\n",
    "    # Reshape and predict\n",
    "    prediction = model.predict(np.expand_dims(embedding, axis=0))\n",
    "    predicted_class = \"AI-generated\" if prediction > 0.5 else \"Human-written\"\n",
    "    \n",
    "    return predicted_class, prediction[0][0]\n",
    "\n",
    "# Sample texts to test\n",
    "sample_texts = [\n",
    "    \"The sunset over the lake was breathtakingly beautiful.\",\n",
    "    \"Utilizing advanced algorithms, this system optimizes resource allocation.\",\n",
    "    \"Artificial intelligence is a rapidly evolving field.\",\n",
    "    \"I love the way you describe the nuances of human emotion.\"\n",
    "]\n",
    "\n",
    "# Test each sample text\n",
    "for text in sample_texts:\n",
    "    predicted_class, confidence = predict_text(text, model, tokenizer, model)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Predicted Class: {predicted_class} (Confidence: {confidence:.2f})\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e37b31f5-29cb-4013-a143-1d405ceed2cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Test each sample text\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m sample_texts:\n\u001b[1;32m---> 31\u001b[0m     predicted_class, confidence \u001b[38;5;241m=\u001b[39m predict_text(text, model, \u001b[43mtokenizer\u001b[49m, bert_model)\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted Class: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (Confidence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfidence\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to preprocess and generate BERT embeddings for a single input\n",
    "def predict_text(text, model, tokenizer, bert_model):\n",
    "    # Preprocess text\n",
    "    cleaned_text = clean_text(text)  # Reuse the clean_text function\n",
    "    lemmatized_text = ' '.join(lemmatize_tokens(word_tokenize(cleaned_text)))\n",
    "\n",
    "    # Generate BERT embeddings\n",
    "    inputs = tokenizer(lemmatized_text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "    embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "\n",
    "    # Reshape and predict\n",
    "    prediction = model.predict(np.expand_dims(embedding, axis=0))\n",
    "    predicted_class = \"AI-generated\" if prediction > 0.5 else \"Human-written\"\n",
    "    \n",
    "    return predicted_class, prediction[0][0]\n",
    "\n",
    "# Sample texts to test\n",
    "sample_texts = [\n",
    "    \"The sunset over the lake was breathtakingly beautiful.\",\n",
    "    \"Utilizing advanced algorithms, this system optimizes resource allocation.\",\n",
    "    \"Artificial intelligence is a rapidly evolving field.\",\n",
    "    \"I love the way you describe the nuances of human emotion.\"\n",
    "]\n",
    "\n",
    "# Test each sample text\n",
    "for text in sample_texts:\n",
    "    predicted_class, confidence = predict_text(text, model, tokenizer, bert_model)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Predicted Class: {predicted_class} (Confidence: {confidence:.2f})\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c8eb71-c482-4b23-a3c7-851b84361ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow_env)",
   "language": "python",
   "name": "tensorflow_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
